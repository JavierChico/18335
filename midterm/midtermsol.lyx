#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble

\renewcommand{\vec}[1]{\mathbf{#1}}

\renewcommand{\labelenumi}{(\alph{enumi})}
\renewcommand{\labelenumii}{(\roman{enumii})}

\newcommand{\tr}{\operatorname{tr}}
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "times" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\topmargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 2
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section*
18.335 Take-Home Midterm Exam: Spring 2022 
\color blue
Solutions
\end_layout

\begin_layout Standard
Posted 1pm Wednesday April 6, due 
\series bold
1pm Friday April 8.
\end_layout

\begin_layout Subsection*
Problem 0: Honor code
\end_layout

\begin_layout Standard
Copy and sign the following in your solutions:
\end_layout

\begin_layout Standard

\emph on
I have not used any resources to complete this exam other than my own 18.335
 notes, the textbook, running my own Julia code, and posted 18.335 course
 materials.
\end_layout

\begin_layout Standard
\begin_inset VSpace 30pt
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Tabular
<lyxtabular version="3" rows="2" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="right" valignment="top">
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset space \hspace{}
\length 30col%
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset CommandInset line
LatexCommand rule
offset "0ex"
width "60col%"
height "1pt"

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="right" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
your signature
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Subsection*
Problem 1: (10+10+8+5 points)
\end_layout

\begin_layout Standard
Consider the two real vectors 
\begin_inset Formula $u=(u_{1},u_{2},\ldots,u_{n})^{T}$
\end_inset

 and 
\begin_inset Formula $v=(v_{1},v_{2},\ldots,v_{n})^{T}$
\end_inset

 and recall that the 1-norm of a vector 
\begin_inset Formula $x$
\end_inset

 is defined as 
\begin_inset Formula $\|x\|_{1}=|x_{1}|+|x_{2}|+\cdots+|x_{n}|$
\end_inset

.
 
\end_layout

\begin_layout Enumerate
Show that computing the dot product 
\begin_inset Formula $f(u,v)=u_{1}v_{1}+u_{2}v_{2}+\cdots+u_{n}v_{n}$
\end_inset

 in floating point arithmetic with left to right summation is backward stable,
 and in fact that 
\begin_inset Formula $\hat{f}(u,v)=f(u+\delta u,v)=f(u,v+\delta v)$
\end_inset

 for some 
\begin_inset Formula $\delta u$
\end_inset

 and 
\begin_inset Formula $\delta v$
\end_inset

 satisfying 
\begin_inset Formula $\|\delta u\|_{1}\leq\|u\|_{1}\mathcal{\mathcal{O}}(\epsilon_{{\rm mach}})$
\end_inset

 and 
\begin_inset Formula $\|\delta v\|_{1}\leq\|v\|_{1}\mathcal{\mathcal{O}}(\epsilon_{{\rm mach}})$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Description

\color blue
Solution: 
\color inherit
By the fundamental theorem of floating point arithmetic, we know that for
 
\begin_inset Formula $1\leq i\leq n$
\end_inset

, 
\begin_inset Formula 
\[
u_{i}\otimes v_{i}=u_{i}v_{i}(1+\delta_{i}),\qquad\text{where}\qquad|\delta_{i}|\leq\epsilon_{{\rm mach}}.
\]

\end_inset

Setting 
\begin_inset Formula $s_{i}=u_{i}v_{i}(1+\delta_{i})$
\end_inset

, the floating point dot product is then given by
\begin_inset Formula 
\[
\hat{f}(u,v)=s_{1}\oplus s_{2}\oplus\cdots\oplus s_{n}.
\]

\end_inset

Now apply the backward stability result for the summation of 
\begin_inset Formula $n$
\end_inset

 floating point numbers that is written down in the lecture notes:
\begin_inset Formula 
\[
\hat{f}(u,v)=\sum_{i=1}^{n}s_{i}(1+\eta_{i}),\qquad\text{where}\qquad|\eta_{i}|\leq n\epsilon_{{\rm mach}}.
\]

\end_inset

Noting that 
\begin_inset Formula $s_{i}(1-\eta_{i})=u_{i}v_{i}(1+\delta_{i})(1+\eta_{i})=u_{i}v_{i}(1+\delta_{i}+\eta_{i}+\delta_{i}\eta_{i})$
\end_inset

, we conclude that 
\begin_inset Formula 
\[
\hat{f}(u,v)=\sum_{i=1}^{n}[u_{i}+u_{i}(\delta_{i}+\eta_{i}+\delta_{i}\eta_{i})]v_{i}=\sum_{i=1}^{n}u_{i}[v_{i}+v_{i}(\delta_{i}+\eta_{i}+\delta_{i}\eta_{i})],
\]

\end_inset

Now, denote 
\begin_inset Formula $\delta u_{i}=u_{i}(\delta_{i}+\eta_{i}+\delta_{i}\eta_{i})$
\end_inset

 and 
\begin_inset Formula $\delta v_{i}=v_{i}(\delta_{i}+\eta_{i}+\delta_{i}\eta_{i})$
\end_inset

 for 
\begin_inset Formula $1\leq i\leq n$
\end_inset

, and apply the bounds for 
\begin_inset Formula $\delta_{i}$
\end_inset

 and 
\begin_inset Formula $\eta_{i}$
\end_inset

 to conclude that 
\begin_inset Formula 
\[
\|\delta u\|_{1}=\sum_{i=1}^{n}|\delta u_{i}|\leq\epsilon_{{\rm mach}}(n+1+n\epsilon_{{\rm mach}})\sum_{i=1}^{n}|u_{i}|=\|u\|_{1}\mathcal{O}(\epsilon_{{\rm mach}}).
\]

\end_inset

This together with an analogous bound for 
\begin_inset Formula $\|\delta v\|_{1}$
\end_inset

 establishes both backward stability statements.
\end_layout

\end_deeper
\begin_layout Enumerate
Calculate the relative condition number of 
\begin_inset Formula $f(u,v)$
\end_inset

 with respect to its first input 
\begin_inset Formula $u$
\end_inset

 in the 1-norm.
\end_layout

\begin_deeper
\begin_layout Description

\color blue
Solution: 
\color inherit
The relative 1-norm condition number of 
\begin_inset Formula $f(u,v)$
\end_inset

 with respect to the first input 
\begin_inset Formula $u$
\end_inset

 is determined by the Jacobian of the linear map 
\begin_inset Formula $g(u)=v^{T}u$
\end_inset

.
 From equation (12.6) in the textbook, we have that
\begin_inset Formula 
\[
\kappa_{f}(u)=\|v^{T}\|_{1}\frac{\|u\|_{1}}{|v^{T}u|}.
\]

\end_inset

Note that the norm of the Jacobian here is the 
\shape italic
\color black
matrix norm of the row vector 
\begin_inset Formula $v_{1}^{T}$
\end_inset


\shape default
, i.e.,
\begin_inset Formula 
\[
\|v^{T}\|_{1}=\sup_{x\in\mathbb{R}^{n}}\frac{|v^{T}x|}{\|x\|_{1}}=\max_{1\leq i\leq n}|v_{i}|.
\]

\end_inset

The second equality follows from equation (3.9) in the textbook (alternatively,
 see the example in the lecture notes about backward stability and condition
 numbers for summation).
 Substituting 
\begin_inset Formula $\|v^{T}\|_{1}=\max_{1\leq i\leq n}|v_{i}|=\|v\|_{\infty}$
\end_inset

 into the expression for 
\begin_inset Formula $\kappa_{f}(u)$
\end_inset

, we conclude that
\begin_inset Formula 
\[
\kappa_{f}(u)=\frac{\|u\|_{1}\|v\|_{\infty}}{|v^{T}u|}.
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
Using your results in parts (a) and (b), derive a bound on the forward error
 in 
\begin_inset Formula $f(u,v)$
\end_inset

.
 You should write the first order term explicitly (don't use 
\begin_inset Quotes eld
\end_inset

big-oh
\begin_inset Quotes erd
\end_inset

 notation).
\end_layout

\begin_deeper
\begin_layout Description

\color blue
Solution: 
\color inherit
By part (a), we know that 
\begin_inset Formula $\hat{f}(u,v)=f(u+\delta u,v)=g(u+\delta u)$
\end_inset

 where 
\begin_inset Formula $\|\delta u\|_{1}\leq\|u\|_{1}\epsilon_{{\rm mach}}(n+1)+\mathcal{O}(\epsilon_{{\rm mach}}^{2})$
\end_inset

.
 Therefore, we have that
\begin_inset Formula 
\[
\frac{|\hat{f}(u,v)-f(u,v)|}{|f(u,v)|}=\frac{|g(u+\delta u)-g(u)|}{|g(u)|}\leq\kappa_{f}(u)\frac{\|\delta u\|_{1}}{\|u\|_{1}}.
\]

\end_inset

Plugging in the results for 
\begin_inset Formula $\kappa_{f}(u)$
\end_inset

 and the bound on 
\begin_inset Formula $\|\delta u\|_{1}$
\end_inset

 and simplifying, we obtain the forward error bound 
\begin_inset Formula 
\[
\frac{|\hat{f}(u,v)-f(u,v)|}{|f(u,v)|}\leq\frac{\|u\|_{1}\|v\|_{\infty}}{|v^{T}u|}\frac{\|\delta u\|_{1}}{\|u\|_{1}}=\frac{\|u\|_{1}\|v\|_{\infty}}{|v^{T}u|}(n+1)\epsilon_{{\rm mach}}+\mathcal{O}(\epsilon_{{\rm mach}}^{2}).
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
Explain why computing the outer product 
\begin_inset Formula $F(u,v)=uv^{T}$
\end_inset

is not backward stable.
\end_layout

\begin_deeper
\begin_layout Description

\color blue
Solution: 
\color inherit
A backward stable algorithm would have to compute a rank-one matrix of the
 form
\begin_inset Formula 
\[
\hat{F}(u,v)=F(u+\delta u,v+\delta v)=(u+\delta u)(v+\delta v)^{T}
\]

\end_inset

with 
\begin_inset Formula $\delta u$
\end_inset

 and 
\begin_inset Formula $\delta v$
\end_inset

 sufficiently small.
 However, the fundamental theorem of floating point arithmetic implies that
 actual computed matrix 
\begin_inset Formula $\hat{F}=\hat{F}(u,v)$
\end_inset

 has entries
\begin_inset Formula 
\[
\hat{F}_{ij}=u_{i}v_{j}(1+\delta_{ij}),\qquad\text{where\qquad|\delta_{ij}|\leq\epsilon_{{\rm mach}}\qquad\text{for\qquad1\leq i,j\leq n.}}
\]

\end_inset

In other words, we have 
\begin_inset Formula $\hat{F}=uv^{T}+\Delta,$
\end_inset

with 
\begin_inset Formula $\|\Delta\|=\mathcal{O}(\epsilon_{{\rm mach}}).$
\end_inset

 But since each entry of 
\begin_inset Formula $\Delta$
\end_inset

 may very independently, in general, 
\begin_inset Formula $\hat{F}$
\end_inset

will 
\series bold
\color black
not 
\series default
be a rank one matrix.
\end_layout

\end_deeper
\begin_layout Subsection*
Problem 2: (9 + 9 + 9 + 6 points)
\end_layout

\begin_layout Standard
Consider a tall skinny matrix 
\begin_inset Formula $X$
\end_inset

 with 
\begin_inset Formula $m$
\end_inset

 rows and 
\begin_inset Formula $n\ll m$
\end_inset

 linearly independent columns.
 Let 
\begin_inset Formula $X=QR$
\end_inset

 be the reduced QR factorization of 
\begin_inset Formula $X$
\end_inset

 and let 
\begin_inset Formula $A=LL^{T}$
\end_inset

 be the Cholesky factorization of the 
\begin_inset Formula $n\times n$
\end_inset

 symmetric positive definite matrix 
\begin_inset Formula $A=X^{T}X$
\end_inset

.
\end_layout

\begin_layout Enumerate
Prove that the triangular factors in the two decompositions are equal, i.e.,
 that 
\begin_inset Formula $R=L^{T}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Description

\color blue
Solution: 
\color black
Since 
\begin_inset Formula $X$
\end_inset

 has linearly independent columns, 
\begin_inset Formula $X$
\end_inset

 has a unique reduced QR factorization, 
\begin_inset Formula $X=QR$
\end_inset

, with positive entries on the diagonal of 
\begin_inset Formula $R$
\end_inset

.
 Moreover, 
\begin_inset Formula $A=X^{T}X$
\end_inset

 is symmetric positive definite and has a unique Cholesky decomposition,
 
\begin_inset Formula $A=LL^{T}$
\end_inset

.
 We calculate directly:
\begin_inset Formula 
\[
A=(QR)^{T}(QR)=R^{T}Q^{T}QR=R^{T}R.
\]

\end_inset

Since 
\begin_inset Formula $R$
\end_inset

 is upper triangular and has positive entries on the diagonal, this is a
 Cholesky decomposition of 
\begin_inset Formula $A$
\end_inset

.
 Since the Cholesky decomposition is unique, it is 
\shape italic
the 
\shape default
Cholesky decomposition of 
\begin_inset Formula $A$
\end_inset

.
 Therefore, 
\begin_inset Formula $R=L^{T}.$
\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate
Using (a), describe an algorithm that computes both 
\begin_inset Formula $Q$
\end_inset

 and 
\begin_inset Formula $R$
\end_inset

 from the Cholesky factorization of 
\begin_inset Formula $A$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Description

\color blue
Solution: 
\color black
From part (a), we have 
\begin_inset Formula $R=L^{T}$
\end_inset

and therefore 
\begin_inset Formula $Q=XR^{-1}=XL^{-T}$
\end_inset

.
 We suggest the following algorithm.

\color inherit
 Given 
\begin_inset Formula $X,$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Step 1) Compute the matrix product 
\begin_inset Formula $A=X^{T}X.$
\end_inset


\end_layout

\begin_layout Itemize
Step 2) Compute the Cholesky factorization 
\begin_inset Formula $A=LL^{T}$
\end_inset

.
\end_layout

\begin_layout Itemize
Step 3) Solve the linear system 
\begin_inset Formula $LQ^{T}=X^{T}$
\end_inset

 (notice, this is a square linear system with 
\begin_inset Formula $m$
\end_inset

 right-hand sides) for 
\begin_inset Formula $Q^{T}$
\end_inset

 by forward substitution.
\end_layout

\begin_layout Standard
The outputs are 
\begin_inset Formula $R=L^{T}$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Enumerate
How many flops does each step from your algorithm in (b) require? Include
 the cost of forming 
\begin_inset Formula $A$
\end_inset

 and computing the Cholesky decomposition.
 You should write the leading order terms explicitly, i.e.
 
\begin_inset Formula $\sim2n^{2}$
\end_inset

, rather than 
\begin_inset Formula $\mathcal{\mathcal{O}}(n^{2})$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Description

\color blue
Solution: 
\color black
In Step 1), each entry of 
\begin_inset Formula $A$
\end_inset

 requires computing a dot product between two vectors of length 
\begin_inset Formula $m$
\end_inset

, which costs 
\begin_inset Formula $\sim2m$
\end_inset

 flops.
 There are 
\begin_inset Formula $n^{2}$
\end_inset

 entries in 
\begin_inset Formula $A$
\end_inset

, but because of symmetry we only need to compute 
\begin_inset Formula $n(n+1)/2\sim n^{2}/2$
\end_inset

 entries in the lower triangular part.
 Therefore the cost of Step 1) is 
\begin_inset Formula $\sim mn^{2}$
\end_inset

 flops.
 In Step 2), the Cholesky factorization of 
\begin_inset Formula $A$
\end_inset

 costs 
\begin_inset Formula $\sim\frac{1}{3}n^{3}$
\end_inset

 flops (see Lecture 23 in the textbook).
 And in Step 3), we solve 
\begin_inset Formula $m$
\end_inset

 square linear systems of dimension 
\begin_inset Formula $n$
\end_inset

 by forward substitution.
 Forward substitution has the same cost as back substitution, namely 
\begin_inset Formula $\sim n^{2}$
\end_inset

 flops (see Lecture 17 in the textbook), so the total cost of Step 3) is
 
\begin_inset Formula $mn^{2}$
\end_inset

 flops.
 The total cost to leading order in 
\begin_inset Formula $n$
\end_inset

 and 
\begin_inset Formula $m$
\end_inset

 is therefore
\begin_inset Formula 
\[
\text{\# flops}\sim2mn^{2}+\frac{1}{3}n^{3}.
\]

\end_inset

Notice that when 
\begin_inset Formula $m\gg n$
\end_inset

, this is comparable to the flop count for both the Gram-Schmidt and Householder
 QR factorizations.
\end_layout

\end_deeper
\begin_layout Enumerate
Explain why the orthogonal factor 
\begin_inset Formula $\hat{Q}$
\end_inset

 computed by the algorithm in (b) in floating point may not be close to
 an orthogonal matrix when 
\begin_inset Formula $X$
\end_inset

 is ill-conditioned.
\end_layout

\begin_deeper
\begin_layout Description

\color blue
Solution: 
\color black
The essence of the problem is that Step 3) applies the triangular transformation
 
\begin_inset Formula $R^{-1}=L^{-T}$
\end_inset

 to orthogonalize the columns of 
\begin_inset Formula $X$
\end_inset

.
 This procedure can be unstable when 
\begin_inset Formula $X$
\end_inset

 is ill-conditioned, as seen in the loss-of-orthogonality suffered by both
 classical and modified Gram-Schmidt methods.
 Furthermore, any errors in the computed triangular factor 
\begin_inset Formula $\hat{R}=\hat{L}^{T}$
\end_inset

 or in the subsequent triangular solve 
\begin_inset Formula $\hat{L}Q^{T}=X^{T}$
\end_inset

 can lead to the potential loss of orthogonality in the columns of the computed
 
\begin_inset Formula $\hat{Q}$
\end_inset

.
 We can make a quantitative argument as follows.
\end_layout

\begin_deeper
\begin_layout Itemize
According to the textbook (Lecture 23), the computed Cholesky factor may
 be innacurate when 
\begin_inset Formula $A$
\end_inset

 is ill-conditioned, with 
\begin_inset Formula $\|\hat{L}-L\|=\|L\|\mathcal{O}(\kappa(A)\epsilon_{{\rm mach}})$
\end_inset

.
 The singular values of 
\begin_inset Formula $A=X^{T}X$
\end_inset

 are just the singular values of 
\begin_inset Formula $X$
\end_inset

 squared, so 
\begin_inset Formula $\kappa(A)=\kappa(X)^{2}$
\end_inset

.
 Therefore, we can write the triangular factor computed in Step 2) as 
\begin_inset Formula $\hat{L}=L+E,$
\end_inset

with
\begin_inset Formula 
\[
\|E\|=\|\hat{L}-L\|=\|L\|\mathcal{O}(\kappa(X)^{2}\epsilon_{{\rm mach}}).
\]

\end_inset


\end_layout

\begin_layout Itemize
In Step 3) we solve 
\begin_inset Formula $(L+E)\hat{Q}^{T}=X^{T}$
\end_inset

.
 If we write 
\begin_inset Formula $\hat{Q}=Q+\Delta Q$
\end_inset

, then we can make the first-order approximation
\begin_inset Formula 
\[
X^{T}=(L+E)(Q+\Delta Q)^{T}\approx LQ^{T}+EQ^{T}+L\Delta Q^{T}
\]

\end_inset

Solving for 
\begin_inset Formula $\Delta Q^{T}$
\end_inset

, taking norms, and using the fact that 
\begin_inset Formula $Q^{T}=L^{-1}X^{T}$
\end_inset

 we obtain a first-order approximation for the error in the computed orthogonal
 factor:
\begin_inset Formula 
\[
\|\Delta Q^{T}\|\approx\|-Q^{T}+L^{-1}(X^{T}+EQ^{T})\|=\|L^{-1}EQ^{T}\|\leq\|L^{-1}\|\|L\|\mathcal{O}(\kappa(X)^{2}\epsilon_{{\rm mach}}).
\]

\end_inset

Since 
\begin_inset Formula $\|L^{-1}\|\|L\|=\kappa(L)=\kappa(X),$
\end_inset

 the last factor is proportional to 
\begin_inset Formula $\mathcal{O}(\kappa(X)^{3}\epsilon_{{\rm mach}})$
\end_inset

.
 Then, to first-order in 
\begin_inset Formula $\epsilon_{{\rm mach}}$
\end_inset

, we have that
\begin_inset Formula 
\[
\|\hat{Q}^{T}\hat{Q}-I\|\approx\|\Delta Q^{T}Q+Q^{T}\Delta Q\|=\mathcal{O}(\kappa(X)^{3}\epsilon_{{\rm mach}}).
\]

\end_inset


\end_layout

\begin_layout Standard
This argument produces only a first-order 
\shape italic
estimate
\shape default
 of the 
\shape italic
worst case
\shape default
 error; it does not produce a rigorous upper or lower bound on the loss
 of orthogonality.
 For example, we have neglected the errors from solving the linear system
 in floating point (which amounts to dropping a term proportional to 
\begin_inset Formula $\kappa(X)\epsilon_{{\rm mach}}$
\end_inset

), as well as the errors commited while computing 
\begin_inset Formula $A$
\end_inset

 in Step 1).
 So what do we see in practice? Check out the attached iJulia notebook to
 see some numerical experiments and related commentary.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard

\series bold
Remark: 
\series default
Although Cholesky-based QR appears unstable, it has excellent communication
 costs.
 Remarkably, the instability explored in (d) can be overcome by, essentially,
 iterating the algorithm three times.
 The result is a state-of-the-art communication-minimizing algorithm! For
 more, see the paper on shifted Cholesky QR at 
\begin_inset CommandInset href
LatexCommand href
name "https://epubs.siam.org/doi/10.1137/18M1218212"
target "https://epubs.siam.org/doi/10.1137/18M1218212"
literal "false"

\end_inset

.
\end_layout

\begin_layout Subsection*
Problem 3: (10 + 18 + 6 points)
\end_layout

\begin_layout Standard
In analogy to the QR algorithm for the standard eigenvalue problem 
\begin_inset Formula $Av=\lambda v$
\end_inset

, the QZ algorithm is used to compute generalized eigenvalues of the generalized
 eigenvalue problem 
\begin_inset Formula $Av=\lambda Bv$
\end_inset

, where 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 are 
\begin_inset Formula $n\times n$
\end_inset

 matrices.
 Like QR, it proceeds in two stages: use orthogonal transformations (e.g.,
 Householder transformations) applied from the left and right to (i) reduce
 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 to upper Hessenberg form and upper triangular form, and (ii) perform QZ
 iterations that drive the subdiagonal entries of the upper Hessenberg matrix
 
\begin_inset Formula $A$
\end_inset

 to zero.
 In this problem, you will work through stage (i) of the QZ algorithm.
\end_layout

\begin_layout Enumerate
Apply a sequence of Householder transformations on the left of 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 that triangularize 
\begin_inset Formula $B.$
\end_inset


\end_layout

\begin_deeper
\begin_layout Description

\color blue
Solution:
\color black
 See the attached iJulia notebook.
\end_layout

\end_deeper
\begin_layout Enumerate
Apply a sequence of Householder transformations on the left and right of
 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 that make 
\begin_inset Formula $A$
\end_inset

 upper Hessenberg and keep 
\begin_inset Formula $B$
\end_inset

 upper triangular.
 (Hint: use transformations from the left to introduce zeros in 
\begin_inset Formula $A$
\end_inset

 and from the right to keep 
\begin_inset Formula $B$
\end_inset

 upper triangular.)
\end_layout

\begin_deeper
\begin_layout Description

\color blue
Solution: 
\color black
See the attached iJulia notebook.
\end_layout

\end_deeper
\begin_layout Enumerate
Is stage (i) backward stable? Give a brief explanation (one or two sentences).
\end_layout

\begin_deeper
\begin_layout Description

\color blue
Solution:
\color inherit
 Yes, stage (i) is backward stable.
 This is because only orthogonal transformations are applied to the inputs
 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 to obtain the, respectively, upper Hessenberg and triangular outputs.
 In homework 2, we saw that applying a product of orthogonal transformations
 to a vector is backward stable, and it is straightforward to apply this
 result to the columns and rows of 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 to show that stage (i) is backward stable.
\end_layout

\end_deeper
\begin_layout Standard
Your answer should include diagrams that show the pattern of zeros and nonzeros
 after each Householder transformation is applied, as well as Julia code
 to execute stage (i).
 You may use Julia code from the notes folder on the course page to compute
 and apply Householder reflectors.
\end_layout

\end_body
\end_document
