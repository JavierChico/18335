{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18.335 pset 2 solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## problem 2b\n",
    "\n",
    "In the problem, you proved that $\\Vert B \\Vert_p \\le \\Vert A \\Vert_p$, where $B$ is any submatrix of $A$, for any induced p-norm (p ≥ 1).\n",
    "\n",
    "I asked you to check your result in Julia — it is a good habit to try out matrix properties on random matrices, both to check for mistakes and to get used to matrix calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×7 Array{Float64,2}:\n",
       " 0.966254   0.154278  0.660569  0.502114   0.395222  0.151819   0.956062\n",
       " 0.680552   0.999574  0.989832  0.307477   0.195687  0.382579   0.0859115\n",
       " 0.301037   0.197895  0.132331  0.66237    0.918919  0.0331439  0.88624\n",
       " 0.279206   0.588467  0.292418  0.306543   0.151264  0.588086   0.220816\n",
       " 0.53973    0.110364  0.571191  0.0817416  0.353622  0.70927    0.561898\n",
       " 0.974273   0.908356  0.234442  0.337053   0.399677  0.391655   0.0767352\n",
       " 0.440267   0.673242  0.30946   0.218332   0.466704  0.785643   0.045229\n",
       " 0.902215   0.742378  0.771771  0.87213    0.287575  0.492945   0.785189\n",
       " 0.0892707  0.740438  0.349118  0.186837   0.326098  0.610411   0.307369\n",
       " 0.113577   0.663616  0.434377  0.887404   0.894945  0.3861     0.00570242"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = rand(10,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.3436419540444628\n",
       " 0.4758216924231012\n",
       " 0.5424170125083305"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra # for opnorm\n",
    "\n",
    "B = A[2:5, 3:7] # a submatrix\n",
    "\n",
    "# the ratio ‖B‖/‖A‖ for the induced p=1,2,∞ norms:\n",
    "[opnorm(B,p) / opnorm(A,p) for p in (1,2,Inf)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay, the ratio is ≤ 1 as desired.\n",
    "\n",
    "**It is okay if you just checked the p=2 norm for a single submatrix.**\n",
    "\n",
    "If we want, we could compute the ratio for all possible non-empty submatrices for p=2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.001404909767735077, 1.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio = Float64[]\n",
    "for a = 1:10, b = a:10, c = 1:7, d = c:7\n",
    "    push!(ratio, opnorm(A[a:b, c:d]) / opnorm(A))\n",
    "end\n",
    "minimum(ratio), maximum(ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, $0.00962 \\le \\Vert B \\Vert_2 / \\Vert A \\Vert_2 \\le 1$ for all possible submatrices of this $A$, consistent with your proved result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## problem 4 (Trefethen 11.2)\n",
    "\n",
    "Here, we want to minimize\n",
    "$$\n",
    "\\int_a^b \\left|\\frac{1}{x} - f(x)\\right|^2 dx\n",
    "$$\n",
    "for an interval $(a,b) = (1,2)$ in part (a) and $(a,b) = (0,1)$ in part (b), over all possible coefficients c of\n",
    "$$\n",
    "f(x) = c_1 e^x + c_2 \\sin(x) + c_3 \\Gamma(x) . \n",
    "$$\n",
    "to at least 1% accuracy.\n",
    "\n",
    "There are several ways to approach this problem, depending on how we want to compute the integrals.\n",
    "\n",
    "On a computer, we typically need to approximate an integral $\\int_a^b g(x)dx$ by a [quadrature rule](https://en.wikipedia.org/wiki/Numerical_integration) of the form:\n",
    "$$\n",
    "\\int_a^b g(x)dx \\approx \\sum_{k=1}^m w_k g(x_k)\n",
    "$$\n",
    "where $x_k$ are the $m$ **quadrature points** and $w_k$ are the **quadrature weights**. We will have more to say on this subject later in 18.06, but for now let's just use the simplest possible quadrature rule — the one that most of you learned when integration was first defined to you, a [Riemann sum](https://en.wikipedia.org/wiki/Riemann_sum):\n",
    "$$\n",
    "\\int_a^b g(x)dx \\approx \\sum_{k=1}^m  g(a + i\\Delta x) \\Delta x,\n",
    "$$\n",
    "where $\\Delta x = (b - a)/N$.  That is, we just approximate the integral as a **sum of rectangles**.  If we choose a large enough $N$, this is sufficiently accurate.\n",
    "\n",
    "Once we do this, the functional minimization problem becomes an ordinary **matrix least-squares** problem:\n",
    "$$\n",
    "\\int_a^b \\left|\\frac{1}{x} - f(x)\\right|^2 dx = \\Vert Ac - b\\Vert_2^2 (\\Delta x)^2\n",
    "$$\n",
    "where $c = (c_1,c_2,c_3)$ (column vector), \n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "e^x_1 & \\sin(x_1) & \\Gamma(x_1) \\\\\n",
    "e^x_2 & \\sin(x_2) & \\Gamma(x_2) \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "e^x_m & \\sin(x_m) & \\Gamma(x_m)\n",
    "\\end{pmatrix}, b = \\begin{pmatrix} 1/x_1 \\\\ 1/x_2 \\\\ \\vdots 1/x_m \\end{pmatrix},\n",
    "$$\n",
    "with $x_k = a + i\\Delta x$.\n",
    "\n",
    "We can then solve this with `c = A \\ b` in Julia (which uses a variant of Householder QR).\n",
    "\n",
    "Let's write a function to construct this solution given $a,b,m$, so that we can plug in different values easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cfit (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using SpecialFunctions # for the Γ(x) function gamma(x)\n",
    "\n",
    "# the fit coefficients c for a given interval (a,b) and a number m of Riemann-sum points\n",
    "function cfit(a,b,m)\n",
    "    Δx = (b-a)/m\n",
    "    x = a .+ (1:m)*Δx\n",
    "    A = [exp.(x) sin.(x) gamma.(x)]\n",
    "    b = 1 ./ x\n",
    "    return A \\ b # the least-square coefficients\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part (a)\n",
    "\n",
    "Here, $(a,b) = (1,2)$.   Let's try evaluating our `cfit` function above for various numbers of points `m` from 10 to $10^6$ and see the convergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6-element Array{Array{Float64,1},1}:\n",
       " [-0.1078660461660563, 0.004992125477160044, 1.2921608135882419]\n",
       " [-0.10777942089180041, 0.008771206004915781, 1.287661305514134]\n",
       " [-0.1077763553565129, 0.009165342313085053, 1.2872236707205442]\n",
       " [-0.10777611040978795, 0.009204892920088487, 1.2871800783321041]\n",
       " [-0.10777608653642387, 0.009208849324636559, 1.287175720849681]\n",
       " [-0.10777608415530587, 0.009209244978506568, 1.2871752851190472]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[cfit(1,2,m) for m in 10 .^ (1:6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, it seems to be converging!\n",
    "\n",
    "In particular, it is converging to $\\boxed{c \\approx (-0.1078, 0.009209, 1.2872)}$.\n",
    "\n",
    "(A more careful analysis shows that the Riemann-sum approximation has an error that converges as $O(1/m)$.   There are **much better** ways to do numerical integration!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part (b)\n",
    "\n",
    "Here, $(a,b) = (0,1)$.  Let's try our `cfit` function again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6-element Array{Array{Float64,1},1}:\n",
       " [0.6508183359559631, -1.925841393331335, 0.9999015363513064]\n",
       " [0.6498620853519183, -1.8963532983984785, 0.9989437018833397]\n",
       " [0.6309184107226293, -1.8337108675743912, 0.9998609606479877]\n",
       " [0.6269075397198692, -1.8204787595543002, 0.9999805673969226]\n",
       " [0.6261104248573651, -1.8178504788404921, 0.999997411674164]\n",
       " [0.6259715730673621, -1.8173922750759735, 0.9999996737972219]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[cfit(0,1,m) for m in 10 .^ (1:6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, it seems to be converging, this time to $\\boxed{c \\approx (0.626, -1.817, 1.0)}$.\n",
    "\n",
    "This is actually the right answer.   But something funny is going on here, and it is worth our time to think about it more carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our function $1/x$ diverges as $x \\to 0^+$, and worse, its **integral diverges**.  That means that we will get\n",
    "$$\n",
    "\\int_a^b \\left|\\frac{1}{x} - f(x)\\right|^2  = \\infty\n",
    "$$\n",
    "*unless* our fit function $f(x)$ **exactly cancels the divergence**.\n",
    "\n",
    "$e^x$ and $\\sin(x)$ are not diverging, but $\\Gamma(x)$ *does* diverge.  Trefethen's hint tells you that $\\Gamma(x)^{-1}$ has slope 1 at $x=0$, which means $\\Gamma(x)^{-1} = x + o(x)$, i.e. $\\Gamma(x) = 1/x + o(x)/x = 1/x + \\mbox{finite}$.  (More generally, one can write a [Laurent series](https://en.wikipedia.org/wiki/Laurent_series) for the Γ function around zero.)   So, $\\Gamma(x)$ has exactly the divergence that we need, and even the right coefficient, so for $a=0$ we must have **exactly** $\\boxed{c_3 = 1}$, which is what we were getting numerically above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth digging deeper — **why didn't our numerics run into trouble?**\n",
    "\n",
    "We got \"lucky\", because I chose a quadrature rule that **didn't evaluate** $f(0)$ — the smallest $x$ I evaluated was $f(\\Delta x)$.   Even so, our **A matrix is becoming badly conditioned** as we increase $m$, since the **third column is blowing up**.  We can compute this by calling `cond(A)` to find the condition number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6-element Array{Float64,1}:\n",
       "   128.2268245943078 \n",
       "   146.28792430783017\n",
       "   379.08689306753394\n",
       "  1146.0959245137424 \n",
       "  3592.3712785121734 \n",
       " 11342.979990053122  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function cfit_cond(a,b,m)\n",
    "    Δx = (b-a)/m\n",
    "    x = a .+ (1:m)*Δx\n",
    "    A = [exp.(x) sin.(x) gamma.(x)]\n",
    "    cond(A)\n",
    "end\n",
    "[cfit_cond(0,1,m) for m in 10 .^ (1:6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckly, **double precision covered for our \"sins\"**.   Even for $m=10^6$, we get a condition number of about $10^4$, which is not enough to prevent us from achieving 1% accuracy.\n",
    "\n",
    "Alternatively, we could have set $c_3 = 1$ analytically and done a **two-parameter fit** by moving the $\\Gamma(x)$ term to the right-hand side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cond(A) = 20.19153545232387\n",
      "cond(A) = 16.75382995902203\n",
      "cond(A) = 16.46483632485854\n",
      "cond(A) = 16.436400349836358\n",
      "cond(A) = 16.433561315633575\n",
      "cond(A) = 16.433277457776853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6-element Array{Array{Float64,1},1}:\n",
       " [0.6498926402534453, -1.9230419083576036, 1.0]\n",
       " [0.627952732095168, -1.8266517509709703, 1.0]\n",
       " [0.626143733632372, -1.8182329520772051, 1.0]\n",
       " [0.6259660174699866, -1.8174011087506348, 1.0]\n",
       " [0.6259482771509939, -1.8173180230331019, 1.0]\n",
       " [0.6259465034315377, -1.8173097154457831, 1.0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit function for a=0 to b, in which gamma coefficient is required to be 1:\n",
    "function cfit(b,m)\n",
    "    a = 0\n",
    "    Δx = (b-a)/m\n",
    "    x = a .+ (1:m)*Δx\n",
    "    A = [exp.(x) sin.(x) ]    # matrix for 2-parameter fit\n",
    "    @show cond(A)\n",
    "    b = 1 ./ x .- gamma.(x)   # Γ(x) term is moved to right-hand side\n",
    "    return [A \\ b; 1] # the least-square coefficients and c₃=1\n",
    "end\n",
    "[cfit(1,m) for m in 10 .^ (1:6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's still converging to the same $\\boxed{c \\approx (0.626, -1.817, 1.0)}$ result as before (where the 1.0 is no longer a fit degree of freedom), but in this case our matrix is well-conditioned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment: computing $\\frac{1}{x} - \\Gamma(x)$\n",
    "\n",
    "Even for the \"clever\" solution where we analytically set $c_3 = 1$, we still need to compute $\\frac{1}{x} - \\Gamma(x)$ for the right-hand side vector at $x = \\Delta x, 2\\Delta x, \\ldots$.   This difference is susceptible to **catastrophic cancellation** if $x$ is very small:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20-element Array{Float64,1}:\n",
       " 0.4864923013312694\n",
       " 0.5674148808493982\n",
       " 0.5762275154045255\n",
       " 0.5771167683760723\n",
       " 0.5772057744325139\n",
       " 0.577214675839059\n",
       " 0.5772155653685331\n",
       " 0.5772156566381454\n",
       " 0.5772156715393066\n",
       " 0.5772151947021484\n",
       " 0.57720947265625\n",
       " 0.5771484375\n",
       " 0.578125\n",
       " 0.578125\n",
       " 0.5\n",
       " 2.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1/x - gamma(x) for x in 0.1 .^ (1:20)] # compute 1/x - Γ(x) for x = 0.1,0.01,…,1e-20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the answer is simply `0.0` for $x \\lesssim 10^{-17}$ — *all* of the significant digits have cancelled, even though the exact answer is:\n",
    "$$\n",
    "\\lim_{x\\to 0^+} \\left[ \\frac{1}{x} - \\Gamma(x) \\right] = \\gamma \\approx 0.57721566490153286060651209\\ldots\n",
    "$$\n",
    "($\\gamma$ is the [Euler–Mascheroni constant](https://en.wikipedia.org/wiki/Euler%E2%80%93Mascheroni_constant))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, **what saved us is double precision** — we only went up to $m=10^6$ or $\\Delta x = 10^{-6}$, for which cancellation \"only\" loses 6 digits … no problem for 1% accuracy.\n",
    "\n",
    "But if we wanted to go to even greater accuracy, we would have a problem.  In order to get around this, we would need a better way to compute the function $1/x - \\Gamma(x)$, much like the cotangent difference you considered in pset 1.  One possibility would be to switch to a [series expansion](http://functions.wolfram.com/GammaBetaErf/Gamma/06/01/01/01/) for small $x$:\n",
    "$$\n",
    "\\frac{1}{x} - \\Gamma(x) = \\gamma - \\frac{1}{6}\\left(3\\gamma^2 + \\frac{\\pi^2}{2}\\right)x + \\cdots\n",
    "$$\n",
    "This is always a bit tricky to implement — you have to decide for what $|x|$ you switch to the series, and how many terms to keep, in order to maintain a given precision.  Many special functions are implemented in this way, stitching together various polynomial and rational-function approximations in different portions of their domain, but it is rather delicate to get right.\n",
    "\n",
    "Sometimes, one can exploit various identities in order to rearrange a formula like this into something more stable.  I'm not seeing anything at first glance.  In any case, you weren't expected to deal with this (since you only needed 1% accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment: Least-squares with integrals\n",
    "\n",
    "It is also interesting to think about how we would solve the \"exact\" integral least-squares problem.   One option is to forget everything we've ever learned about linear algebra and just take partial derivatives of the integral with respect to parameters, but that's no fun.  Instead, let's think of the \"matrix\" $A$ as the **linear operator** that takes coefficients $c = (c_1,c_2,c_3)$ and gives you a *function* $f(x)$:\n",
    "$$\n",
    "Ac = c_1 e^x + c_2 \\sin(x) + c_3 \\Gamma(x) .\n",
    "$$\n",
    "This is like a \"matrix with infinitely many rows\", since it maps $c\\in \\mathbb{C}^4$ into a space of *functions*.\n",
    "\n",
    "Now, suppose we know that minimizing $\\Vert Ac-b \\Vert_2$ leads to the normal equations $A^* A c = A^* b$, but what is $A^*$?   The key is to realize that we are implicitly using an [inner product](https://en.wikipedia.org/wiki/Inner_product_space) (\"dot product\") given by $$f \\cdot g = \\int_a^b \\bar{f}(x) g(x)dx$$, from which we have our $L_2$ norm $\\Vert f \\Vert^2 = \\int_a^b |f(x)|^2 dx$.    In this sense, $A^*$ is the [dual](https://en.wikipedia.org/wiki/Dual_space) linear operator that, given a function $f$, computes the \"dot\" product with the columns:\n",
    "$$\n",
    "A^* f = \\begin{pmatrix} e^x \\cdot f(x) \\\\ \\sin(x) \\cdot f(x) \\\\ \\Gamma(x) \\cdot f(x) \\end{pmatrix} .\n",
    "$$\n",
    "All of our algebra for least-squares still works if we define $A^*$ in this way!  So, we end up with $3 \\times 3$ normal equations:\n",
    "$$\n",
    "\\underbrace{\\begin{pmatrix} e^x \\cdot e^x & e^x \\cdot \\sin(x) & e^x \\cdot \\Gamma(x) \\\\\n",
    "               \\sin(x) \\cdot e^x & \\sin(x) \\cdot \\sin(x) & \\sin(x) \\cdot \\Gamma(x) \\\\\n",
    "               \\Gamma(x) \\cdot e^x & \\Gamma(x) \\cdot \\sin(x) & \\Gamma(x) \\cdot \\Gamma(x) \\end{pmatrix}}_{A^* A} c = \\underbrace{\\begin{pmatrix} e^x \\cdot \\frac{1}{x} \\\\ \\sin(x) \\cdot \\frac{1}{x} \\\\ \\Gamma(x) \\cdot \\frac{1}{x} \\end{pmatrix}}_{A^* (1/x)}\n",
    "$$\n",
    "\n",
    "Now that we've formulated it this way, we still need to compute the integrals, but we can compute them any way we like.  For example, we can compute them using the [QuadGK package](https://github.com/JuliaMath/QuadGK.jl) for Julia, which uses fancy \"adaptive\" methods that use a different number of points depending what the function is doing.   The main point is, we know need to know precisely how the numerical integration works, since it all happens \"inside\" $A^* A$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Array{Float64,2}:\n",
       " 23.6045   4.48756   4.32093\n",
       "  4.48756  0.916525  0.881419\n",
       "  4.32093  0.881419  0.852602"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 3.059116539645953\n",
       " 0.6593299064355118\n",
       " 0.6398719692098005"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " -0.10777608389080662\n",
       "  0.009209288940199732\n",
       "  1.2871752367047251"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using QuadGK # for the quadgk numerical integration function\n",
    "\n",
    "function cfit2(a,b; rtol=1e-12) # continuous-x fit, doing integrals to relative tolerance rtol\n",
    "    A = [exp,sin,gamma]\n",
    "    AᵀA = [quadgk(x -> f(x)*g(x), a,b, rtol=rtol)[1] for f in A, g in A]\n",
    "    Aᵀb = [quadgk(x -> f(x)/x, a,b, rtol=rtol)[1] for f in A]\n",
    "    display(AᵀA)\n",
    "    display(Aᵀb)\n",
    "    return AᵀA \\ Aᵀb # the least-square coefficients\n",
    "end\n",
    "\n",
    "cfit2(1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, our results seemed to be converging to $\\boxed{c \\approx (-0.1078, 0.009209, 1.2872)}$, and now we see that indeed this is correct for the \"exact\" integrals (to 10+ digits) as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to do the $\\int_0^1$ fit this way, however, we will have a problem because the integrals involving $\\Gamma(x)$ and $1/x$ no longer converge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mDomainError with 3.5601181736115222e-307:\u001b[39m\n\u001b[91mintegrand produced Inf in the interval (0.0, 7.120236347223045e-307)\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mDomainError with 3.5601181736115222e-307:\u001b[39m\n\u001b[91mintegrand produced Inf in the interval (0.0, 7.120236347223045e-307)\u001b[39m",
      "",
      "Stacktrace:",
      " [1] evalrule(::var\"#13#17\"{typeof(gamma),typeof(exp)}, ::Float64, ::Float64, ::Array{Float64,1}, ::Array{Float64,1}, ::Array{Float64,1}, ::typeof(norm)) at /Users/stevenj/.julia/packages/QuadGK/czbUH/src/evalrule.jl:41",
      " [2] adapt(::Function, ::Array{QuadGK.Segment{Float64,Float64,Float64},1}, ::Float64, ::Float64, ::Int64, ::Array{Float64,1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Int64, ::Float64, ::Float64, ::Int64, ::typeof(norm)) at /Users/stevenj/.julia/packages/QuadGK/czbUH/src/adapt.jl:45",
      " [3] do_quadgk(::var\"#13#17\"{typeof(gamma),typeof(exp)}, ::Tuple{Int64,Int64}, ::Int64, ::Nothing, ::Float64, ::Int64, ::typeof(norm)) at /Users/stevenj/.julia/packages/QuadGK/czbUH/src/adapt.jl:35",
      " [4] (::QuadGK.var\"#28#29\"{Nothing,Float64,Int64,Int64,typeof(norm)})(::Function, ::Tuple{Int64,Int64}, ::Function) at /Users/stevenj/.julia/packages/QuadGK/czbUH/src/adapt.jl:179",
      " [5] handle_infinities at /Users/stevenj/.julia/packages/QuadGK/czbUH/src/adapt.jl:113 [inlined]",
      " [6] #quadgk#27 at /Users/stevenj/.julia/packages/QuadGK/czbUH/src/adapt.jl:177 [inlined]",
      " [7] (::var\"#12#16\"{Float64,Int64,Int64})(::Tuple{typeof(gamma),typeof(exp)}) at ./none:0",
      " [8] iterate at ./generator.jl:47 [inlined]",
      " [9] collect_to!(::Array{Float64,2}, ::Base.Generator{Base.Iterators.ProductIterator{Tuple{Array{Function,1},Array{Function,1}}},var\"#12#16\"{Float64,Int64,Int64}}, ::Int64, ::Tuple{Tuple{typeof(exp),Int64},Tuple{typeof(exp),Int64}}) at ./array.jl:732",
      " [10] collect_to_with_first!(::Array{Float64,2}, ::Float64, ::Base.Generator{Base.Iterators.ProductIterator{Tuple{Array{Function,1},Array{Function,1}}},var\"#12#16\"{Float64,Int64,Int64}}, ::Tuple{Tuple{typeof(exp),Int64},Tuple{typeof(exp),Int64}}) at ./array.jl:710",
      " [11] collect(::Base.Generator{Base.Iterators.ProductIterator{Tuple{Array{Function,1},Array{Function,1}}},var\"#12#16\"{Float64,Int64,Int64}}) at ./array.jl:691",
      " [12] cfit2(::Int64, ::Int64; rtol::Float64) at ./In[9]:5",
      " [13] cfit2(::Int64, ::Int64) at ./In[9]:4",
      " [14] top-level scope at In[10]:1",
      " [15] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
     ]
    }
   ],
   "source": [
    "cfit2(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we *must* set $c_3=1$ analytically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Array{Float64,2}:\n",
       " 3.19453   0.909331\n",
       " 0.909331  0.272676"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 0.34706840472875855\n",
       " 0.0736563323865726"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       "  0.6259463063551046\n",
       " -1.8173087923915738\n",
       "  1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# special case for a=0 where c₃=1\n",
    "function cfit2(b; rtol=1e-12) # continuous-x fit, doing integrals to relative tolerance rtol\n",
    "    a = 0\n",
    "    A = [exp,sin] # Γ(x) is moved to right-hand side\n",
    "    AᵀA = [quadgk(x -> f(x)*g(x), a,b, rtol=rtol)[1] for f in A, g in A]\n",
    "    Aᵀb = [quadgk(x -> f(x)*(1/x-gamma(x)), a,b, rtol=rtol)[1] for f in A]\n",
    "    display(AᵀA)\n",
    "    display(Aᵀb)\n",
    "    return [AᵀA \\ Aᵀb; 1] # the least-square coefficients including c₃=1\n",
    "end\n",
    "\n",
    "cfit2(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, these results are consistent with the $\\boxed{c \\approx (0.626, -1.817, 1.0)}$ results that we obtained in part (b).   Again, we see that our $2\\times 2$ matrix $A^*A$ is well-conditioned.\n",
    "\n",
    "(Our accuracy might still be limited by the $A^* (1/x - \\Gamma(x))$ computation because of cancellation error, as noted above, unless we find a better implementation of this special function.   However, we are helped by the fact that `quadgk` never evaluates the integrand exactly at its endpoints, and it uses a very fast-converging quadrature algorithm that doesn't even get particularly close to the endpoints for smooth integrands.)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.5.4",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
